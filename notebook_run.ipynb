{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ec6b00-9879-4a76-ab53-55b0afe33434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.24.4)\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.11/site-packages (8.3.5)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.11/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pytest) (23.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.11/site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install tabulate numpy pytest torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a385d3-1afa-449e-b8a2-90a520651216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed64757a-6696-4e9c-8fb7-4bb251547e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: tensor([[ 1.0714,     nan, -0.0535],\n",
      "        [ 0.0869,  0.0046, -0.6433]])\n",
      "Optimized output: tensor([[ 1.0714,  0.0604, -0.0535],\n",
      "        [ 0.0869,  0.0046, -0.6433]])\n",
      "Expected output: tensor([[ 1.0714,  0.0604, -0.0535],\n",
      "        [ 0.0869,  0.0046, -0.6433]])\n",
      "Optimized matches original: False\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name    target                                                  args        kwargs\n",
      "-------------  ------  ------------------------------------------------------  ----------  --------\n",
      "placeholder    a       a                                                       ()          {}\n",
      "placeholder    b       b                                                       ()          {}\n",
      "placeholder    c       c                                                       ()          {}\n",
      "call_function  mul_3   <built-in method mul of type object at 0x7f235161ff00>  (a, b)      {}\n",
      "call_function  mul_4   <built-in method mul of type object at 0x7f235161ff00>  (mul_3, c)  {}\n",
      "output         output  output                                                  (mul_4,)    {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "import operator\n",
    "\n",
    "\n",
    "class SqrtAssociativePass:\n",
    "    \"\"\"Implements the rewrite pattern: (A⊙√B)⊙(√B⊙C) => A⊙B⊙C\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mul_patterns = {operator.mul, torch.mul, \"mul\"}\n",
    "\n",
    "    def __call__(self, module):\n",
    "        traced = symbolic_trace(module)\n",
    "        graph = traced.graph\n",
    "\n",
    "        # Keep track of nodes to delete\n",
    "        nodes_to_delete = set()\n",
    "\n",
    "        # Find all multiplication nodes\n",
    "        for node in graph.nodes:\n",
    "            is_mul_fn = node.op == \"call_function\" and node.target in self.mul_patterns\n",
    "            is_mul_mth = node.op == \"call_method\" and node.target in (\n",
    "                \"mul\", \"__mul__\")\n",
    "            if not ((is_mul_fn or is_mul_mth) and len(node.args) == 2):\n",
    "                continue\n",
    "\n",
    "            # Check if this is a multiplication of two terms\n",
    "            if len(node.args) != 2:\n",
    "                continue\n",
    "\n",
    "            lhs, rhs = node.args[0], node.args[1]\n",
    "\n",
    "            # Check if lhs is a multiplication with sqrt (function or method)\n",
    "            if isinstance(lhs, torch.fx.Node):\n",
    "                is_lhs_mul_fn = lhs.op == \"call_function\" and lhs.target in self.mul_patterns\n",
    "                is_lhs_mul_mth = lhs.op == \"call_method\" and lhs.target in (\n",
    "                    \"mul\", \"__mul__\")\n",
    "                if not ((is_lhs_mul_fn or is_lhs_mul_mth) and len(lhs.args) == 2):\n",
    "                    continue\n",
    "\n",
    "                lhs_lhs, lhs_rhs = lhs.args[0], lhs.args[1]\n",
    "\n",
    "                # Check if lhs_rhs is a sqrt (function or method)\n",
    "                is_sqrt_fn = lhs_rhs.op == \"call_function\" and lhs_rhs.target == torch.sqrt\n",
    "                is_sqrt_mth = lhs_rhs.op == \"call_method\" and lhs_rhs.target == \"sqrt\"\n",
    "                if not (is_sqrt_fn or is_sqrt_mth):\n",
    "                    continue\n",
    "\n",
    "                sqrt_arg = lhs_rhs.args[0]\n",
    "\n",
    "                # Check if rhs is a multiplication with sqrt (fn or method)\n",
    "                is_rhs_mul_fn = isinstance(\n",
    "                    rhs, torch.fx.Node) and rhs.op == \"call_function\" and rhs.target in self.mul_patterns\n",
    "                is_rhs_mul_mth = isinstance(\n",
    "                    rhs, torch.fx.Node) and rhs.op == \"call_method\" and rhs.target in (\"mul\", \"__mul__\")\n",
    "                if not ((is_rhs_mul_fn or is_rhs_mul_mth) and len(rhs.args) == 2):\n",
    "                    continue\n",
    "\n",
    "                rhs_lhs, rhs_rhs = rhs.args[0], rhs.args[1]\n",
    "\n",
    "                # Check if rhs_lhs is the same sqrt (function or method)\n",
    "                is_sqrt2_fn = rhs_lhs.op == \"call_function\" and rhs_lhs.target == torch.sqrt\n",
    "                is_sqrt2_mth = rhs_lhs.op == \"call_method\" and rhs_lhs.target == \"sqrt\"\n",
    "                if not ((is_sqrt2_fn or is_sqrt2_mth) and rhs_lhs.args[0] is sqrt_arg):\n",
    "                    continue\n",
    "\n",
    "                # Create new multiplication chain: A⊙B⊙C\n",
    "                with graph.inserting_before(node):\n",
    "                    # First multiply A and B\n",
    "                    ab = graph.call_function(\n",
    "                        torch.mul, args=(lhs_lhs, sqrt_arg), kwargs=node.kwargs)\n",
    "                    # Then multiply with C\n",
    "                    new_node = graph.call_function(\n",
    "                        torch.mul, args=(ab, rhs_rhs), kwargs=node.kwargs)\n",
    "\n",
    "                    # Replace the original node with the new one\n",
    "                    node.replace_all_uses_with(new_node)\n",
    "\n",
    "                    # Add nodes to delete set\n",
    "                    nodes_to_delete.update(\n",
    "                        [node, lhs, rhs, lhs_rhs, rhs_lhs])\n",
    "\n",
    "        # Delete nodes in reverse order to avoid dependency issues\n",
    "        for node in reversed(list(graph.nodes)):\n",
    "            if node in nodes_to_delete:\n",
    "                try:\n",
    "                    graph.erase_node(node)\n",
    "                except Exception as e:\n",
    "                    # Skip if node is already deleted\n",
    "                    pass\n",
    "\n",
    "        graph.lint()\n",
    "        new_mod = GraphModule(traced, graph)\n",
    "        new_mod.recompile()\n",
    "        return new_mod\n",
    "\n",
    "\n",
    "# Test code\n",
    "def test_sqrt_associative():\n",
    "    class TestModule(torch.nn.Module):\n",
    "        def forward(self, a, b, c):\n",
    "            # Implement the (A⊙√B)⊙(√B⊙C) pattern\n",
    "            sqrt_b = torch.sqrt(b)\n",
    "            return (a * sqrt_b) * (sqrt_b * c)\n",
    "\n",
    "    # Create test inputs\n",
    "    a = torch.randn(2, 3) + 1  # Add 1 to ensure positive values\n",
    "    b = torch.randn(2, 3) + 1  # Add 1 to ensure positive values\n",
    "    c = torch.randn(2, 3)\n",
    "\n",
    "    # Original module\n",
    "    original_module = TestModule()\n",
    "    original_output = original_module(a, b, c)\n",
    "\n",
    "    # Apply optimization\n",
    "    try:\n",
    "        optimized_module = SqrtAssociativePass()(TestModule())\n",
    "        optimized_output = optimized_module(a, b, c)\n",
    "\n",
    "        # Manually compute expected optimized result\n",
    "        expected_output = a * b * c\n",
    "\n",
    "        # Verify results\n",
    "        print(\"Original output:\", original_output)\n",
    "        print(\"Optimized output:\", optimized_output)\n",
    "        print(\"Expected output:\", expected_output)\n",
    "        print(\"Optimized matches original:\", torch.allclose(\n",
    "            original_output, optimized_output))\n",
    "        print(\"Optimized matches expected:\", torch.allclose(\n",
    "            optimized_output, expected_output))\n",
    "\n",
    "        # Print optimized graph\n",
    "        print(\"\\nOptimized graph:\")\n",
    "        optimized_module.graph.print_tabular()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during optimization: {e}\")\n",
    "        # Print original graph to assist debugging\n",
    "        print(\"\\nOriginal graph:\")\n",
    "        original_module = symbolic_trace(TestModule())\n",
    "        original_module.graph.print_tabular()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sqrt_associative()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc39110-e768-48dd-999b-e3118d6f52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FX before rewrite ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %bitwise_left_shift : [num_users=1] = call_function[target=torch.bitwise_left_shift](args = (%x, 1), kwargs = {})\n",
      "    %sum_1 : [num_users=1] = call_method[target=sum](args = (%bitwise_left_shift,), kwargs = {})\n",
      "    return sum_1\n",
      "=== FX after rewrite ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %sum_2 : [num_users=1] = call_function[target=torch.sum](args = (%x,), kwargs = {})\n",
      "    %bitwise_left_shift_1 : [num_users=1] = call_function[target=torch.bitwise_left_shift](args = (%sum_2, 1), kwargs = {})\n",
      "    return bitwise_left_shift_1\n",
      "input:\n",
      " tensor([[ 1, 14,  6, 10],\n",
      "        [ 0,  7, 13, 10],\n",
      "        [12, 15, 11, 12],\n",
      "        [ 8, 13,  8, 13]], dtype=torch.int32)\n",
      "Uncompiled output: tensor(306)\n",
      "Compiled output: tensor(306)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import operator\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "\n",
    "def swap_bitshift_reducesum(gm: torch.fx.GraphModule):\n",
    "    \"\"\"Rewrite Commutative: ReduceSum(BitShift(x)) → BitShift(ReduceSum(x)) bit shift.\"\"\"\n",
    "    BITSHIFT_FNS = {torch.bitwise_left_shift, operator.lshift}\n",
    "    # Only left shift is included as it distributes over sum (e.g., sum(x << k) == sum(x) << k).\n",
    "    # Right shift does NOT distribute over addition and can lead to incorrect rewrites.\n",
    "    modified = False\n",
    "\n",
    "    for node in list(gm.graph.nodes):\n",
    "        is_sum = (\n",
    "            (node.op == \"call_method\" and node.target == \"sum\") or\n",
    "            (node.op == \"call_function\" and node.target in (torch.sum, sum))\n",
    "        )\n",
    "\n",
    "        if not is_sum or len(node.args) == 0:\n",
    "            continue\n",
    "\n",
    "        input_node = node.args[0]\n",
    "        is_shift = (\n",
    "            (input_node.op == \"call_function\" and input_node.target in BITSHIFT_FNS) or\n",
    "            (input_node.op == \"call_method\" and input_node.target == \"__lshift__\")\n",
    "        )\n",
    "        if not is_shift:\n",
    "            continue\n",
    "\n",
    "        x, shift = input_node.args\n",
    "\n",
    "        with gm.graph.inserting_before(node):\n",
    "            new_sum = gm.graph.call_function(\n",
    "                torch.sum,\n",
    "                args=(x,),\n",
    "                kwargs=node.kwargs\n",
    "            )\n",
    "\n",
    "            new_bitshift = gm.graph.call_function(\n",
    "                torch.bitwise_left_shift,\n",
    "                args=(new_sum, shift),\n",
    "                kwargs=input_node.kwargs\n",
    "            )\n",
    "\n",
    "            node.replace_all_uses_with(new_bitshift)\n",
    "            # Erase old nodes\n",
    "            gm.graph.erase_node(node)\n",
    "            gm.graph.erase_node(input_node)\n",
    "            modified = True\n",
    "\n",
    "    if modified:\n",
    "        # Clean up any dead nodes and rebuild the module\n",
    "        gm.graph.lint()\n",
    "        new_gm = torch.fx.GraphModule(gm, gm.graph)\n",
    "        new_gm.recompile()\n",
    "        return new_gm\n",
    "    else:\n",
    "        return gm\n",
    "\n",
    "\n",
    "def run_bitshift_swap():\n",
    "    class BitShiftSwap(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            # Original subgraph: ReduceSum(BitShift(A))\n",
    "            t = torch.bitwise_left_shift(x, 1)  # BitShift(x,1)\n",
    "            out = t.sum()                       # ReduceSum(t)\n",
    "            return out\n",
    "\n",
    "    gm = symbolic_trace(BitShiftSwap().eval())\n",
    "    print(\"=== FX before rewrite ===\")\n",
    "    print(gm.graph)\n",
    "\n",
    "    gm = swap_bitshift_reducesum(gm)\n",
    "    print(\"=== FX after rewrite ===\")\n",
    "    print(gm.graph)\n",
    "\n",
    "    compiled_gm = torch.compile(gm, backend=\"inductor\")\n",
    "\n",
    "    x = torch.randint(0, 16, (4, 4), dtype=torch.int32)\n",
    "\n",
    "    out_uncompiled = gm(x)\n",
    "    out = compiled_gm(x)\n",
    "\n",
    "    print(\"input:\\n\", x)\n",
    "    print(\"Uncompiled output:\", out_uncompiled)\n",
    "    print(\"Compiled output:\", out)\n",
    "\n",
    "    assert torch.equal(out_uncompiled, out), \"Outputs do not match!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_bitshift_swap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d3d79a-1354-41c7-8273-c165f3ff244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x: tensor([[ 0.6909, -0.7713, -0.5386],\n",
      "        [-0.7573, -0.6858, -0.4379]])\n",
      "Original output: tensor(0.0821)\n",
      "Optimized output: tensor(0.0821)\n",
      "Expected output: tensor(0.0821)\n",
      "Optimized matches original: True\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name    target                                                  args      kwargs\n",
      "-------------  ------  ------------------------------------------------------  --------  --------\n",
      "placeholder    x       x                                                       ()        {}\n",
      "call_function  sum_1   <built-in method sum of type object at 0x7f235161ff00>  (x,)      {}\n",
      "call_function  exp_1   <built-in method exp of type object at 0x7f235161ff00>  (sum_1,)  {}\n",
      "output         output  output                                                  (exp_1,)  {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "import operator\n",
    "\n",
    "\n",
    "class CommutativePass:\n",
    "    \"\"\"Implements the rewrite pattern: ReduceProd(Exp(A)) => Exp(ReduceSum(A))\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reduce_prod_patterns = {torch.prod, \"prod\"}\n",
    "        self.exp_patterns = {torch.exp, \"exp\"}\n",
    "\n",
    "    def __call__(self, module):\n",
    "        traced = symbolic_trace(module)\n",
    "        graph = traced.graph\n",
    "\n",
    "        # Keep track of nodes to delete\n",
    "        nodes_to_delete = set()\n",
    "\n",
    "        # Find all prod nodes (function and method)\n",
    "        for node in graph.nodes:\n",
    "            is_prod = (\n",
    "                (node.op == \"call_function\" and node.target in self.reduce_prod_patterns) or (\n",
    "                    node.op == \"call_method\" and node.target == \"prod\")\n",
    "            )\n",
    "            if not is_prod or len(node.args) < 1:\n",
    "                continue\n",
    "\n",
    "            # Get the argument of reduce_prod\n",
    "            arg = node.args[0]\n",
    "\n",
    "            # Check if the argument is an exp operation\n",
    "            if not (\n",
    "                isinstance(arg, torch.fx.Node) and (\n",
    "                    (arg.op == \"call_function\" and arg.target in self.exp_patterns)\n",
    "                    or (arg.op == \"call_method\" and arg.target == \"exp\")\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            # Get the argument of exp\n",
    "            exp_arg = arg.args[0]\n",
    "\n",
    "            # Extract only the reduction dimensions from prod\n",
    "            reduce_kwargs = {}\n",
    "            for key in ('dim', 'keepdim', 'dtype'):\n",
    "                if key in node.kwargs:\n",
    "                    reduce_kwargs[key] = node.kwargs[key]\n",
    "\n",
    "            # Create new operations: Exp(ReduceSum(A))\n",
    "            with graph.inserting_before(node):\n",
    "                # Sum only over the same dimensions as the original prod\n",
    "                reduce_sum = graph.call_function(\n",
    "                    torch.sum, args=(exp_arg,), kwargs=reduce_kwargs)\n",
    "                # Then compute Exp(ReduceSum(A))\n",
    "                new_node = graph.call_function(\n",
    "                    torch.exp, args=(reduce_sum,))\n",
    "\n",
    "                # Replace the original node with the new one\n",
    "                node.replace_all_uses_with(new_node)\n",
    "\n",
    "                # Add nodes to delete set\n",
    "                nodes_to_delete.update([node, arg])\n",
    "\n",
    "        # Delete nodes in reverse order to avoid dependency issues\n",
    "        for node in reversed(list(graph.nodes)):\n",
    "            if node in nodes_to_delete:\n",
    "                try:\n",
    "                    graph.erase_node(node)\n",
    "                except Exception as e:\n",
    "                    # Skip if node is already deleted\n",
    "                    pass\n",
    "\n",
    "        graph.lint()\n",
    "        new_module = GraphModule(traced, graph)\n",
    "        new_module.recompile()\n",
    "        return new_module\n",
    "\n",
    "\n",
    "# Test code\n",
    "def test_commutative():\n",
    "    class TestModule(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            # Implement the ReduceProd(Exp(A)) pattern\n",
    "            exp_x = torch.exp(x)\n",
    "            return torch.prod(exp_x)\n",
    "\n",
    "    # Create test inputs\n",
    "    x = torch.randn(2, 3)\n",
    "\n",
    "    # Original module\n",
    "    original_module = TestModule()\n",
    "    original_output = original_module(x)\n",
    "\n",
    "    # Apply optimization\n",
    "    try:\n",
    "        optimized_module = CommutativePass()(TestModule())\n",
    "        optimized_output = optimized_module(x)\n",
    "\n",
    "        # Manually compute expected optimized result\n",
    "        expected_output = torch.exp(torch.sum(x))\n",
    "\n",
    "        # Verify results\n",
    "        print(\"Input x:\", x)\n",
    "        print(\"Original output:\", original_output)\n",
    "        print(\"Optimized output:\", optimized_output)\n",
    "        print(\"Expected output:\", expected_output)\n",
    "        print(\"Optimized matches original:\", torch.allclose(\n",
    "            original_output, expected_output))\n",
    "        print(\"Optimized matches expected:\", torch.allclose(\n",
    "            optimized_output, expected_output))\n",
    "\n",
    "        # Print optimized graph\n",
    "        print(\"\\nOptimized graph:\")\n",
    "        optimized_module.graph.print_tabular()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during optimization: {e}\")\n",
    "        # Print original graph to assist debugging\n",
    "        print(\"\\nOriginal graph:\")\n",
    "        original_module = symbolic_trace(TestModule())\n",
    "        original_module.graph.print_tabular()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_commutative()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49c925bf-08cb-4623-86d6-4a1b5161736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: tensor([[ 0.4573, -0.6412,  0.2246],\n",
      "        [-0.2847, -1.4171,  0.0560]])\n",
      "Optimized output: tensor([[ 0.4573, -0.6412,  0.2246],\n",
      "        [-0.2847, -1.4171,  0.0560]])\n",
      "Optimized matches original: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name    target                                                  args        kwargs\n",
      "-------------  ------  ------------------------------------------------------  ----------  --------\n",
      "placeholder    a       a                                                       ()          {}\n",
      "placeholder    b       b                                                       ()          {}\n",
      "placeholder    c       c                                                       ()          {}\n",
      "call_function  add_1   <built-in method add of type object at 0x7f235161ff00>  (b, c)      {}\n",
      "call_function  mul_2   <built-in method mul of type object at 0x7f235161ff00>  (a, add_1)  {}\n",
      "output         output  output                                                  (mul_2,)    {}\n",
      "Original output: tensor([[-0.0558,  0.1934,  1.3936],\n",
      "        [ 1.3067,  0.0779,  1.9390]])\n",
      "Optimized output: tensor([[-0.0558,  0.1934,  1.3936],\n",
      "        [ 1.3067,  0.0779,  1.9390]])\n",
      "Optimized matches original: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name    target                                                  args        kwargs\n",
      "-------------  ------  ------------------------------------------------------  ----------  --------\n",
      "placeholder    a       a                                                       ()          {}\n",
      "placeholder    b       b                                                       ()          {}\n",
      "placeholder    c       c                                                       ()          {}\n",
      "call_function  add_1   <built-in method add of type object at 0x7f235161ff00>  (b, c)      {}\n",
      "call_function  mul_2   <built-in method mul of type object at 0x7f235161ff00>  (a, add_1)  {}\n",
      "output         output  output                                                  (mul_2,)    {}\n",
      "Original output: tensor([[ 0.6217,  6.4067, -0.2156],\n",
      "        [ 0.3592, -3.3783,  0.3703]])\n",
      "Optimized output: tensor([[ 0.6217,  6.4067, -0.2156],\n",
      "        [ 0.3592, -3.3783,  0.3703]])\n",
      "Optimized matches original: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name    target                                                  args        kwargs\n",
      "-------------  ------  ------------------------------------------------------  ----------  --------\n",
      "placeholder    a       a                                                       ()          {}\n",
      "placeholder    b       b                                                       ()          {}\n",
      "placeholder    c       c                                                       ()          {}\n",
      "call_function  add_1   <built-in method add of type object at 0x7f235161ff00>  (b, c)      {}\n",
      "call_function  mul_2   <built-in method mul of type object at 0x7f235161ff00>  (a, add_1)  {}\n",
      "output         output  output                                                  (mul_2,)    {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "import operator\n",
    "import pytest\n",
    "\n",
    "\n",
    "class DistributiveRulePass:\n",
    "    \"\"\"Implement distributive rule: A ⊙ C + A ⊙ B → A ⊙ (B + C)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Possible representations for addition and multiplication\n",
    "        self.add_patterns = {operator.add, torch.add, \"add\", \"__add__\"}\n",
    "        self.mul_patterns = {operator.mul, torch.mul, \"mul\", \"__mul__\"}\n",
    "\n",
    "    def _is_add_node(self, node):\n",
    "        if not isinstance(node, torch.fx.Node):\n",
    "            return False\n",
    "        return (node.op == \"call_function\" and node.target in self.add_patterns) or \\\n",
    "               (node.op == \"call_method\" and node.target in self.add_patterns)\n",
    "\n",
    "    def _is_mul_node(self, node):\n",
    "        if not isinstance(node, torch.fx.Node):\n",
    "            return False\n",
    "        return (node.op == \"call_function\" and node.target in self.mul_patterns) or \\\n",
    "            (node.op == \"call_method\" and node.target in self.mul_patterns)\n",
    "\n",
    "    def __call__(self, module):\n",
    "        # Symbolically trace the module\n",
    "        traced = symbolic_trace(module)\n",
    "        graph = traced.graph\n",
    "\n",
    "        # First collect all matching “A*C + A*B” nodes\n",
    "        matches = []\n",
    "        for node in graph.nodes:\n",
    "            if not self._is_add_node(node) or len(node.args) != 2:\n",
    "                continue\n",
    "\n",
    "            lhs, rhs = node.args[0], node.args[1]\n",
    "\n",
    "            # Both sides must be multiplication nodes\n",
    "            if not (self._is_mul_node(lhs) and self._is_mul_node(rhs)):\n",
    "                continue\n",
    "            if len(lhs.args) != 2 or len(rhs.args) != 2:\n",
    "                continue\n",
    "\n",
    "            # Find common factor in any operand position\n",
    "            # A*C + A*B  or  C*A + B*A  or  A*C + B*A  or  C*A + A*B\n",
    "            A, B, C = None, None, None\n",
    "            for potential_A in lhs.args:\n",
    "                if potential_A in rhs.args:\n",
    "                    A = potential_A\n",
    "                    # Get remaining terms from both sides\n",
    "                    C = lhs.args[0] if lhs.args[1] is A else lhs.args[1]\n",
    "                    B = rhs.args[0] if rhs.args[1] is A else rhs.args[1]\n",
    "                    break\n",
    "            if A is None:\n",
    "                continue\n",
    "\n",
    "            matches.append((node, A, B, C, lhs, rhs))\n",
    "\n",
    "        # Apply each matched transformation, then erase the old nodes\n",
    "        nodes_to_delete = set()\n",
    "        for add_node, A, B, C, lhs_node, rhs_node in matches:\n",
    "            # A*(B+C)\n",
    "            with graph.inserting_before(add_node):\n",
    "                sum_bc = graph.call_function(torch.add, args=(B, C))\n",
    "                fused = graph.call_function(torch.mul, args=(A, sum_bc))\n",
    "                add_node.replace_all_uses_with(fused)\n",
    "                nodes_to_delete.update({add_node, lhs_node, rhs_node})\n",
    "\n",
    "        # Erase dead nodes\n",
    "        for node in reversed(list(graph.nodes)):\n",
    "            if node in nodes_to_delete and not node.users:\n",
    "                graph.erase_node(node)\n",
    "\n",
    "        graph.lint()\n",
    "\n",
    "        new_mod = GraphModule(traced, graph)\n",
    "        new_mod.recompile()\n",
    "        return new_mod\n",
    "\n",
    "# Test code\n",
    "\n",
    "\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self, variation):\n",
    "        super().__init__()\n",
    "        self.variation = variation\n",
    "\n",
    "    def forward(self, a, b, c):\n",
    "        # Implement the A ⊙ C + A ⊙ B pattern here\n",
    "        if self.variation == 0:\n",
    "            return (c * a) + (b * a)\n",
    "        elif self.variation == 1:\n",
    "            return (a * c) + (b * a)\n",
    "        else:\n",
    "            return (c * a) + (a * b)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"variation\", [0, 1, 2])\n",
    "def test_distributive_rule(variation):\n",
    "    # Create test inputs\n",
    "    a = torch.randn(2, 3)\n",
    "    b = torch.randn(2, 3)\n",
    "    c = torch.randn(2, 3)\n",
    "\n",
    "    # Original module\n",
    "    model = TestModule(variation).eval()\n",
    "    original_output = model(a, b, c)\n",
    "\n",
    "    # Apply optimization\n",
    "    try:\n",
    "        optimized_module = DistributiveRulePass()(model)\n",
    "        optimized_output = optimized_module(a, b, c)\n",
    "\n",
    "        # Verify results\n",
    "        print(\"Original output:\", original_output)\n",
    "        print(\"Optimized output:\", optimized_output)\n",
    "        print(\"Optimized matches original:\", torch.allclose(\n",
    "            original_output, optimized_output))\n",
    "\n",
    "        # Print optimized graph\n",
    "        print(\"\\nOptimized graph:\")\n",
    "        optimized_module.graph.print_tabular()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during optimization: {e}\")\n",
    "        # Print original graph to assist debugging\n",
    "        print(\"\\nOriginal graph:\")\n",
    "        original_module = symbolic_trace(TestModule())\n",
    "        original_module.graph.print_tabular()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for v in [0, 1, 2]:\n",
    "        test_distributive_rule(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8aee8c0-3a68-4718-8c19-bcc28847904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing variation 0\n",
      "Original output: tensor([[-5.9558,  1.2854, -0.1099],\n",
      "        [ 0.3256,  0.3261,  1.8915]])\n",
      "Optimized output: tensor([[-5.9558,  1.2854, -0.1099],\n",
      "        [ 0.3256,  0.3261,  1.8915]])\n",
      "Expected output: tensor([[-5.9558,  1.2854, -0.1099],\n",
      "        [ 0.3256,  0.3261,  1.8915]])\n",
      "Optimized matches original: True\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name       target                                                        args            kwargs\n",
      "-------------  ---------  ------------------------------------------------------------  --------------  --------\n",
      "placeholder    a          a                                                             ()              {}\n",
      "placeholder    b          b                                                             ()              {}\n",
      "call_function  ones_like  <built-in method ones_like of type object at 0x7f235161ff00>  (b,)            {}\n",
      "call_function  add_1      <built-in method add of type object at 0x7f235161ff00>        (b, ones_like)  {}\n",
      "call_function  mul_1      <built-in method mul of type object at 0x7f235161ff00>        (a, add_1)      {}\n",
      "output         output     output                                                        (mul_1,)        {}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing variation 1\n",
      "Original output: tensor([[-1.0881,  0.0648,  1.2577],\n",
      "        [-0.6346,  2.2088,  0.1734]])\n",
      "Optimized output: tensor([[-1.0881,  0.0648,  1.2577],\n",
      "        [-0.6346,  2.2088,  0.1734]])\n",
      "Expected output: tensor([[-1.0881,  0.0648,  1.2577],\n",
      "        [-0.6346,  2.2088,  0.1734]])\n",
      "Optimized matches original: True\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name       target                                                        args            kwargs\n",
      "-------------  ---------  ------------------------------------------------------------  --------------  --------\n",
      "placeholder    a          a                                                             ()              {}\n",
      "placeholder    b          b                                                             ()              {}\n",
      "call_function  ones_like  <built-in method ones_like of type object at 0x7f235161ff00>  (b,)            {}\n",
      "call_function  add_1      <built-in method add of type object at 0x7f235161ff00>        (b, ones_like)  {}\n",
      "call_function  mul_1      <built-in method mul of type object at 0x7f235161ff00>        (a, add_1)      {}\n",
      "output         output     output                                                        (mul_1,)        {}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing variation 2\n",
      "Original output: tensor([[-1.8097,  0.2579,  0.0305],\n",
      "        [-0.7582,  1.9026, -0.5351]])\n",
      "Optimized output: tensor([[-1.8097,  0.2579,  0.0305],\n",
      "        [-0.7582,  1.9026, -0.5351]])\n",
      "Expected output: tensor([[-1.8097,  0.2579,  0.0305],\n",
      "        [-0.7582,  1.9026, -0.5351]])\n",
      "Optimized matches original: True\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name       target                                                        args            kwargs\n",
      "-------------  ---------  ------------------------------------------------------------  --------------  --------\n",
      "placeholder    a          a                                                             ()              {}\n",
      "placeholder    b          b                                                             ()              {}\n",
      "call_function  ones_like  <built-in method ones_like of type object at 0x7f235161ff00>  (b,)            {}\n",
      "call_function  add_1      <built-in method add of type object at 0x7f235161ff00>        (b, ones_like)  {}\n",
      "call_function  mul_1      <built-in method mul of type object at 0x7f235161ff00>        (a, add_1)      {}\n",
      "output         output     output                                                        (mul_1,)        {}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing variation 3\n",
      "Original output: tensor([[-3.8191,  0.0220, -3.8204],\n",
      "        [-0.4363, -0.1361, -0.0566]])\n",
      "Optimized output: tensor([[-3.8191,  0.0220, -3.8204],\n",
      "        [-0.4363, -0.1361, -0.0566]])\n",
      "Expected output: tensor([[-3.8191,  0.0220, -3.8204],\n",
      "        [-0.4363, -0.1361, -0.0566]])\n",
      "Optimized matches original: True\n",
      "Optimized matches expected: True\n",
      "\n",
      "Optimized graph:\n",
      "opcode         name       target                                                        args            kwargs\n",
      "-------------  ---------  ------------------------------------------------------------  --------------  --------\n",
      "placeholder    a          a                                                             ()              {}\n",
      "placeholder    b          b                                                             ()              {}\n",
      "call_function  ones_like  <built-in method ones_like of type object at 0x7f235161ff00>  (b,)            {}\n",
      "call_function  add_1      <built-in method add of type object at 0x7f235161ff00>        (b, ones_like)  {}\n",
      "call_function  mul_1      <built-in method mul of type object at 0x7f235161ff00>        (a, add_1)      {}\n",
      "output         output     output                                                        (mul_1,)        {}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "import operator\n",
    "import pytest\n",
    "\n",
    "\n",
    "class DistributiveRule2Pass:\n",
    "    \"\"\"Implement distributive rule 2: A + A ⊙ B → A ⊙ (B + 1)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Possible representations for addition and multiplication\n",
    "        self.add_patterns = {operator.add, torch.add, \"add\", \"__add__\"}\n",
    "        self.mul_patterns = {operator.mul, torch.mul, \"mul\", \"__mul__\"}\n",
    "\n",
    "    def _is_add_node(self, node):\n",
    "        # Check if node is a torch.fx.Node first\n",
    "        if not isinstance(node, torch.fx.Node):\n",
    "            return False\n",
    "        return (node.op == \"call_function\" and node.target in self.add_patterns) or \\\n",
    "               (node.op == \"call_method\" and node.target in self.add_patterns)\n",
    "\n",
    "    def _is_mul_node(self, node):\n",
    "        # Check if node is a torch.fx.Node first\n",
    "        if not isinstance(node, torch.fx.Node):\n",
    "            return False\n",
    "        return (node.op == \"call_function\" and node.target in self.mul_patterns) or \\\n",
    "               (node.op == \"call_method\" and node.target in self.mul_patterns)\n",
    "\n",
    "    def __call__(self, module):\n",
    "        # Symbolically trace the module\n",
    "        traced = symbolic_trace(module)\n",
    "        graph = traced.graph\n",
    "\n",
    "        # First collect all matching \"A + A*B\" nodes\n",
    "        matches = []\n",
    "        for node in graph.nodes:\n",
    "            if not self._is_add_node(node) or len(node.args) != 2:\n",
    "                continue\n",
    "\n",
    "            lhs, rhs = node.args[0], node.args[1]\n",
    "\n",
    "            # Check pattern: A + (A * B)\n",
    "            if self._is_mul_node(rhs) and len(rhs.args) == 2:\n",
    "                if lhs is rhs.args[0]:  # A + (A * B)\n",
    "                    matches.append((node, lhs, rhs, rhs.args[1]))\n",
    "                elif lhs is rhs.args[1]:  # A + (B * A)\n",
    "                    matches.append((node, lhs, rhs, rhs.args[0]))\n",
    "\n",
    "            # Check pattern: (A * B) + A\n",
    "            if self._is_mul_node(lhs) and len(lhs.args) == 2:\n",
    "                if rhs is lhs.args[0]:  # (A * B) + A\n",
    "                    matches.append((node, rhs, lhs, lhs.args[1]))\n",
    "                elif rhs is lhs.args[1]:  # (B * A) + A\n",
    "                    matches.append((node, rhs, lhs, lhs.args[0]))\n",
    "\n",
    "        # Apply each matched transformation, then erase the old nodes\n",
    "        nodes_to_delete = set()\n",
    "        for add_node, a_term, mul_node, b_term in matches:\n",
    "            # A*(B+1)\n",
    "            with graph.inserting_before(add_node):\n",
    "                # First create a tensor of ones matching B's shape\n",
    "                one_node = graph.call_function(torch.ones_like, args=(b_term,))\n",
    "\n",
    "                # Then compute B + 1\n",
    "                b_plus_one = graph.call_function(torch.add, args=(b_term, one_node))\n",
    "\n",
    "                # Finally compute A * (B + 1)\n",
    "                fused = graph.call_function(torch.mul, args=(a_term, b_plus_one))\n",
    "\n",
    "                add_node.replace_all_uses_with(fused)\n",
    "                nodes_to_delete.update({add_node, mul_node})\n",
    "\n",
    "        # Erase dead nodes\n",
    "        for node in reversed(list(graph.nodes)):\n",
    "            if node in nodes_to_delete and not node.users:\n",
    "                graph.erase_node(node)\n",
    "\n",
    "        graph.lint()\n",
    "\n",
    "        new_mod = GraphModule(traced, graph)\n",
    "        new_mod.recompile()\n",
    "        return new_mod\n",
    "\n",
    "\n",
    "# Test code\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self, variation):\n",
    "        super().__init__()\n",
    "        self.variation = variation\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        # Implement the A + A*B pattern in different variations\n",
    "        if self.variation == 0:\n",
    "            return a + a * b  # A + (A * B)\n",
    "        elif self.variation == 1:\n",
    "            return a + b * a  # A + (B * A)\n",
    "        elif self.variation == 2:\n",
    "            return a * b + a  # (A * B) + A\n",
    "        else:\n",
    "            return b * a + a  # (B * A) + A\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"variation\", [0, 1, 2, 3])\n",
    "def test_distributive_rule2(variation):\n",
    "    # Create test inputs\n",
    "    a = torch.randn(2, 3)\n",
    "    b = torch.randn(2, 3)\n",
    "\n",
    "    # Original module\n",
    "    model = TestModule(variation).eval()\n",
    "    original_output = model(a, b)\n",
    "\n",
    "    # Apply optimization\n",
    "    try:\n",
    "        optimized_module = DistributiveRule2Pass()(model)\n",
    "        optimized_output = optimized_module(a, b)\n",
    "\n",
    "        # Manually compute expected optimized result\n",
    "        expected_output = a * (b + 1)\n",
    "\n",
    "        # Verify results\n",
    "        print(f\"Testing variation {variation}\")\n",
    "        print(\"Original output:\", original_output)\n",
    "        print(\"Optimized output:\", optimized_output)\n",
    "        print(\"Expected output:\", expected_output)\n",
    "        print(\"Optimized matches original:\", torch.allclose(original_output, optimized_output))\n",
    "        print(\"Optimized matches expected:\", torch.allclose(optimized_output, expected_output))\n",
    "\n",
    "        # Print optimized graph\n",
    "        print(\"\\nOptimized graph:\")\n",
    "        optimized_module.graph.print_tabular()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during optimization: {e}\")\n",
    "        # Print original graph to assist debugging\n",
    "        print(\"\\nOriginal graph:\")\n",
    "        traced = symbolic_trace(model)\n",
    "        traced.graph.print_tabular()\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for v in range(4):\n",
    "        test_distributive_rule2(v)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb55cb6d-e742-4bfc-b9c7-3d19667ac5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Graph ===\n",
      "graph():\n",
      "    %a : [num_users=2] = placeholder[target=A]\n",
      "    %b : [num_users=1] = placeholder[target=B]\n",
      "    %reciprocal : [num_users=1] = call_function[target=torch.reciprocal](args = (%a,), kwargs = {})\n",
      "    %mul : [num_users=1] = call_function[target=operator.mul](args = (%a, %b), kwargs = {})\n",
      "    %reciprocal_1 : [num_users=1] = call_function[target=torch.reciprocal](args = (%mul,), kwargs = {})\n",
      "    %mul_1 : [num_users=1] = call_function[target=operator.mul](args = (%reciprocal, %reciprocal_1), kwargs = {})\n",
      "    return mul_1\n",
      "=== Rewritten Graph ===\n",
      "graph():\n",
      "    %a : [num_users=1] = placeholder[target=A]\n",
      "    %b : [num_users=1] = placeholder[target=B]\n",
      "    %reciprocal : [num_users=1] = call_function[target=torch.reciprocal](args = (%a,), kwargs = {})\n",
      "    %square : [num_users=1] = call_function[target=torch.square](args = (%reciprocal,), kwargs = {})\n",
      "    %reciprocal_2 : [num_users=1] = call_function[target=torch.reciprocal](args = (%b,), kwargs = {})\n",
      "    %mul_2 : [num_users=1] = call_function[target=torch.mul](args = (%square, %reciprocal_2), kwargs = {})\n",
      "    return mul_2\n",
      "Original output: tensor([0.0833, 0.0125, 0.0057])\n",
      "Optimized output: tensor([0.0833, 0.0125, 0.0057])\n",
      "Compiled output: tensor([0.0833, 0.0125, 0.0057])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import operator\n",
    "from torch.fx import GraphModule, symbolic_trace\n",
    "\n",
    "\n",
    "def swap_recip_associative(gm: GraphModule):\n",
    "    \"\"\"Rewrite Recip(A)*Recip(A*B) → square(Recip(A)) * Recip(B).\"\"\"\n",
    "    nodes_to_delete = set()\n",
    "    modified = False\n",
    "\n",
    "    for node in list(gm.graph.nodes):\n",
    "        # 1) match outer multiplication\n",
    "        if node.op != \"call_function\" or node.target not in (torch.mul, operator.mul):\n",
    "            continue\n",
    "        lhs, rhs = node.args\n",
    "        if not (is_reciprocal(lhs) and is_reciprocal(rhs)):\n",
    "            continue\n",
    "\n",
    "        # 2) figure out which side is Recip(A) vs Recip(A*B)\n",
    "        a_node = b_node = recip_a_node = mul_node = None\n",
    "        arg1, arg2 = get_recip_arg(lhs), get_recip_arg(rhs)\n",
    "\n",
    "        # case A*B on rhs\n",
    "        if (isinstance(arg2, torch.fx.Node)\n",
    "                and arg2.op == \"call_function\"\n",
    "                and arg2.target in (torch.mul, operator.mul)\n",
    "                and arg1 in arg2.args):\n",
    "            recip_a_node = lhs\n",
    "            mul_node = arg2\n",
    "            a_node = arg1\n",
    "            b_node = arg2.args[1] if arg2.args[0] is arg1 else arg2.args[0]\n",
    "\n",
    "        # case A*B on lhs\n",
    "        elif (isinstance(arg1, torch.fx.Node)\n",
    "              and arg1.op == \"call_function\"\n",
    "              and arg1.target in (torch.mul, operator.mul)\n",
    "              and arg2 in arg1.args):\n",
    "            recip_a_node = rhs\n",
    "            mul_node = arg1\n",
    "            a_node = arg2\n",
    "            b_node = arg1.args[1] if arg1.args[0] is arg2 else arg1.args[0]\n",
    "        else:\n",
    "            continue  # no match\n",
    "\n",
    "        # 3) insert the new ops before the old `node`\n",
    "        with gm.graph.inserting_before(node):\n",
    "            square_recip = gm.graph.call_function(\n",
    "                torch.square, args=(recip_a_node,))\n",
    "            recip_b = gm.graph.call_function(\n",
    "                torch.reciprocal, args=(b_node,))\n",
    "            new_mul = gm.graph.call_function(\n",
    "                torch.mul, args=(square_recip, recip_b))\n",
    "\n",
    "        # 4) redirect uses, mark old nodes for deletion\n",
    "        node.replace_all_uses_with(new_mul)\n",
    "        nodes_to_delete.update({node, mul_node, lhs, rhs})\n",
    "        modified = True\n",
    "\n",
    "    # 5) bulk‑erase dead nodes in reverse order\n",
    "    if modified:\n",
    "        for n in reversed(list(gm.graph.nodes)):\n",
    "            if n in nodes_to_delete and not n.users:\n",
    "                gm.graph.erase_node(n)\n",
    "        gm.graph.lint()\n",
    "        new_module = GraphModule(gm, gm.graph)\n",
    "        new_module.recompile()\n",
    "        return new_module\n",
    "\n",
    "    return gm\n",
    "\n",
    "\n",
    "def is_reciprocal(node):\n",
    "    if node.op != \"call_function\":\n",
    "        return False\n",
    "    if node.target == torch.reciprocal:\n",
    "        return True\n",
    "    if node.target == operator.truediv:\n",
    "        return is_constant_one(node.args[0])\n",
    "    if node.target in (torch.pow, operator.pow):\n",
    "        return len(node.args) >= 2 and float(node.args[1]) == -1.0\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_recip_arg(node):\n",
    "    if node.target == torch.reciprocal:\n",
    "        return node.args[0]\n",
    "    if node.target == operator.truediv:\n",
    "        return node.args[1]\n",
    "    if node.target in (torch.pow, operator.pow) and float(node.args[1]) == -1.0:\n",
    "        return node.args[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_constant_one(node):\n",
    "    return (node.op == \"call_function\"\n",
    "            and node.target == torch.tensor\n",
    "            and isinstance(node.args[0], (int, float))\n",
    "            and float(node.args[0]) == 1.0)\n",
    "\n",
    "\n",
    "def run_associative_swap():\n",
    "    class RecipAssociativeModel(torch.nn.Module):\n",
    "        def forward(self, A, B):\n",
    "            # Implements: Recip(A) ⊙ Recip(A ⊙ B)\n",
    "            recip_a = torch.reciprocal(A)  # 1/A\n",
    "            a_mul_b = A * B                # A*B\n",
    "            recip_a_mul_b = torch.reciprocal(a_mul_b)  # 1/(A*B)\n",
    "            return recip_a * recip_a_mul_b  # (1/A) * (1/(A*B))\n",
    "\n",
    "    gm = symbolic_trace(RecipAssociativeModel().eval())\n",
    "\n",
    "    print(\"=== Original Graph ===\")\n",
    "    print(gm.graph)\n",
    "\n",
    "    optimized = swap_recip_associative(gm)\n",
    "    print(\"=== Rewritten Graph ===\")\n",
    "    print(optimized.graph)\n",
    "\n",
    "    compiled_gm = torch.compile(optimized, backend='inductor')\n",
    "\n",
    "    A = torch.tensor([2.0, 4.0, 5.0])\n",
    "    B = torch.tensor([3.0, 5.0, 7.0])\n",
    "\n",
    "    original_output = gm(A, B)\n",
    "    optimized_output = optimized(A, B)\n",
    "    compiled_output = compiled_gm(A, B)\n",
    "\n",
    "    print(\"Original output:\", original_output)\n",
    "    print(\"Optimized output:\", optimized_output)\n",
    "    print(\"Compiled output:\", compiled_output)\n",
    "\n",
    "    assert torch.allclose(original_output, optimized_output,\n",
    "                          rtol=1e-5), \"Outputs do not match!\"\n",
    "    assert torch.allclose(original_output, compiled_output,\n",
    "                          rtol=1e-5), \"Outputs do not match!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_associative_swap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf04b993-4235-4ab2-b9b5-8e3f153c774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Fusion ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %add : [num_users=1] = call_function[target=torch.add](args = (%x, 1), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.relu](args = (%add,), kwargs = {})\n",
      "    %sigmoid : [num_users=1] = call_function[target=torch.sigmoid](args = (%relu,), kwargs = {})\n",
      "    return sigmoid\n",
      "=== After Fusion ===\n",
      "graph():\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %fused_function : [num_users=1] = call_function[target=__main__.fused_function](args = (%x,), kwargs = {})\n",
      "    return fused_function\n",
      "Ref Output: tensor([[0.6061, 0.5000, 0.8335, 0.6420],\n",
      "        [0.6649, 0.5932, 0.5594, 0.9262],\n",
      "        [0.7824, 0.7213, 0.8650, 0.8978]])\n",
      "Fused Output: tensor([[0.6061, 0.5000, 0.8335, 0.6420],\n",
      "        [0.6649, 0.5932, 0.5594, 0.9262],\n",
      "        [0.7824, 0.7213, 0.8650, 0.8978]])\n",
      "Compiled Output: tensor([[0.6061, 0.5000, 0.8335, 0.6420],\n",
      "        [0.6649, 0.5932, 0.5594, 0.9262],\n",
      "        [0.7824, 0.7213, 0.8650, 0.8978]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "import operator\n",
    "\n",
    "ELEMENTWISE_OPS = {\n",
    "    torch.add, torch.sub, torch.mul, torch.div,\n",
    "    torch.relu, torch.sigmoid, torch.tanh,\n",
    "    torch.neg, torch.exp, torch.log,\n",
    "    operator.add, operator.sub, operator.mul, operator.truediv\n",
    "}\n",
    "\n",
    "METHOD_OPS = {\"relu\", \"sigmoid\", \"tanh\", \"add\", \"mul\", \"div\", \"sub\"}\n",
    "\n",
    "\n",
    "def is_elementwise(node):\n",
    "    if getattr(node.target, \"__name__\", None) == \"fused_function\":\n",
    "        return False\n",
    "    return (node.op == \"call_function\" and node.target in ELEMENTWISE_OPS) or \\\n",
    "           (node.op == \"call_method\" and node.target in METHOD_OPS)\n",
    "\n",
    "\n",
    "def find_elementwise_chains(graph):\n",
    "    chains = []\n",
    "    visited = set()\n",
    "\n",
    "    for node in graph.nodes:\n",
    "        if node in visited:\n",
    "            continue\n",
    "        if not is_elementwise(node):\n",
    "            continue\n",
    "\n",
    "        chain = [node]\n",
    "        visited.add(node)\n",
    "        current = node\n",
    "\n",
    "        while True:\n",
    "            users = list(current.users)\n",
    "            if len(users) != 1:\n",
    "                break\n",
    "\n",
    "            next_node = users[0]\n",
    "            if not is_elementwise(next_node) or next_node in visited:\n",
    "                break\n",
    "\n",
    "            only_depends_on_current = all(\n",
    "                not (isinstance(arg, torch.fx.Node) and arg.op !=\n",
    "                     \"get_attr\" and arg is not current)\n",
    "                for arg in next_node.args\n",
    "            )\n",
    "\n",
    "            if not only_depends_on_current:\n",
    "                break\n",
    "\n",
    "            chain.append(next_node)\n",
    "            visited.add(next_node)\n",
    "            current = next_node\n",
    "\n",
    "        if len(chain) >= 2:\n",
    "            chains.append(chain)\n",
    "    return chains\n",
    "\n",
    "\n",
    "def generate_fused_fn(chain, gm):\n",
    "    # Pre-capture tensor constants\n",
    "    constants = {}\n",
    "\n",
    "    # Check if this chain is safe to fuse\n",
    "    for i, node in enumerate(chain):\n",
    "        if node.op == \"call_function\" and len(node.args) >= 2:\n",
    "            second_arg = node.args[1]\n",
    "            if isinstance(second_arg, torch.fx.Node) and second_arg.op == \"get_attr\":\n",
    "                constants[(i, 1)] = getattr(gm, second_arg.target)\n",
    "            elif not isinstance(second_arg, torch.fx.Node):\n",
    "                constants[(i, 1)] = second_arg\n",
    "            else:\n",
    "                return None  # No fusion if second arg is another variable node\n",
    "\n",
    "    def fused_function(x):\n",
    "        result = x\n",
    "        for i, node in enumerate(chain):\n",
    "            if node.op == \"call_function\":\n",
    "                if len(node.args) == 1:  # unary op\n",
    "                    result = node.target(result)\n",
    "                elif len(node.args) >= 2:  # binary op\n",
    "                    if (i, 1) in constants:\n",
    "                        result = node.target(result, constants[(i, 1)])\n",
    "                    else:\n",
    "                        second_arg = node.args[1]\n",
    "                        result = node.target(result, second_arg)\n",
    "\n",
    "            elif node.op == \"call_method\":\n",
    "                method = getattr(result, node.target)\n",
    "                if len(node.args) == 0:\n",
    "                    result = method()\n",
    "                else:\n",
    "                    method_args = node.args[1:]\n",
    "                    result = method(*method_args)\n",
    "\n",
    "        return result\n",
    "\n",
    "    fused_function.__name__ = \"fused_function\"\n",
    "    fused_function.flops_per_element = len(chain)\n",
    "    fused_function.is_fused_function = True\n",
    "    return fused_function\n",
    "\n",
    "\n",
    "def fuse_elementwise_chains(gm: GraphModule):\n",
    "    graph = gm.graph\n",
    "    chains = find_elementwise_chains(graph)\n",
    "\n",
    "    for chain in chains:\n",
    "        # If the head node is no longer in the graph (erased by a prior fusion), skip\n",
    "        if chain[0] not in graph.nodes:\n",
    "            continue\n",
    "\n",
    "        first = chain[0]\n",
    "        last = chain[-1]\n",
    "        input_val = first.args[0]\n",
    "\n",
    "        fused_fn = generate_fused_fn(chain, gm)\n",
    "        if fused_fn is None:\n",
    "            continue\n",
    "\n",
    "        with graph.inserting_before(first):\n",
    "            fused = graph.call_function(fused_fn, args=(input_val,))\n",
    "\n",
    "        last.replace_all_uses_with(fused)\n",
    "\n",
    "        for node in reversed(chain):\n",
    "            graph.erase_node(node)\n",
    "\n",
    "    gm.recompile()\n",
    "    return gm\n",
    "\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(torch.relu(torch.add(x, 1)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = TestModel()\n",
    "    traced = symbolic_trace(model)\n",
    "\n",
    "    print(\"=== Before Fusion ===\")\n",
    "    print(traced.graph)\n",
    "\n",
    "    optimized = fuse_elementwise_chains(traced)\n",
    "\n",
    "    print(\"=== After Fusion ===\")\n",
    "    print(optimized.graph)\n",
    "\n",
    "    x = torch.randn(3, 4)\n",
    "    ref_out = model(x)\n",
    "    opt_out = optimized(x)\n",
    "\n",
    "    print(\"Ref Output:\", ref_out)\n",
    "    print(\"Fused Output:\", opt_out)\n",
    "\n",
    "    assert torch.allclose(ref_out, opt_out, rtol=1e-4), \"Mismatch in outputs\"\n",
    "\n",
    "    compiled = torch.compile(optimized)\n",
    "    compiled_out = compiled(x)\n",
    "    print(\"Compiled Output:\", compiled_out)\n",
    "    assert torch.allclose(compiled_out, ref_out, rtol=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9cbcfac-c2ca-44b2-b85c-57d7c4f5bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_all_rewrites(gm: torch.fx.GraphModule):\n",
    "    # Commutative rules\n",
    "    gm = swap_bitshift_reducesum(gm)\n",
    "    gm = CommutativePass()(gm)\n",
    "\n",
    "    # Associative rules\n",
    "    gm = swap_recip_associative(gm)\n",
    "    gm = SqrtAssociativePass()(gm)\n",
    "\n",
    "    # Distributive rules\n",
    "    gm = DistributiveRulePass()(gm)\n",
    "    gm = DistributiveRule2Pass()(gm)\n",
    "\n",
    "    return gm\n",
    "\n",
    "\n",
    "def optimize_graph(gm: torch.fx.GraphModule):\n",
    "    gm = apply_all_rewrites(gm)\n",
    "    gm = fuse_elementwise_chains(gm)\n",
    "    return gm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb22534-8e33-40f5-b2e8-c743b82ec9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── EVALUATION SECTION ────────────────────────────────────\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.fx import symbolic_trace, GraphModule\n",
    "from torch.fx.passes.shape_prop import ShapeProp\n",
    "from typing import Callable, Dict, Tuple, List\n",
    "\n",
    "# ─── Helpers ────────────────────────────────────────────────────────────────\n",
    "def _numel_from_meta(node):\n",
    "    return int(np.prod(node.meta[\"tensor_meta\"].shape))\n",
    "\n",
    "def _numel_from_meta_simple(shape: tuple) -> int:\n",
    "    return int(np.prod(shape))\n",
    "\n",
    "# ─── Zero‑cost ops ─────────────────────────────────────────────────────────\n",
    "_ZERO_COST = {\n",
    "    torch.ones_like,\n",
    "    torch.zeros_like,\n",
    "    torch.tensor,\n",
    "    torch.flatten,    # view only\n",
    "}\n",
    "\n",
    "# ─── Recognize both Python and ATen sum/prod ────────────────────────────────\n",
    "_SUM_FNS  = { torch.ops.aten.sum, torch.sum }\n",
    "_PROD_FNS = { torch.ops.aten.prod, torch.prod }\n",
    "\n",
    "# ─── ATen mapping: specific operators → flop functions ──────────────────────\n",
    "ATEN_FLOP_MAP: Dict[Callable, Callable] = {\n",
    "    # element‑wise\n",
    "    torch.ops.aten.add.Tensor:  lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.sub.Tensor:  lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.mul.Tensor:  lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.div.Tensor:  lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.neg:         lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.exp:         lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.log:         lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.sqrt:        lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.sigmoid:     lambda n: 2 * _numel_from_meta(n),\n",
    "    torch.ops.aten.tanh:        lambda n: 2 * _numel_from_meta(n),\n",
    "    torch.ops.aten.square:      lambda n: _numel_from_meta(n),\n",
    "\n",
    "    # constant factories\n",
    "    torch.ops.aten.ones_like:   lambda n: 0,\n",
    "    torch.ops.aten.zeros_like:  lambda n: 0,\n",
    "\n",
    "    # matmul\n",
    "    torch.ops.aten.matmul:      lambda n: 2\n",
    "                                  * n.args[0].shape[0]\n",
    "                                  * n.args[0].shape[1]\n",
    "                                  * n.args[1].shape[-1],\n",
    "\n",
    "    # reductions via ATen\n",
    "    torch.ops.aten.sum:         lambda n: _numel_from_meta(n.args[0]),\n",
    "    torch.ops.aten.prod:        lambda n: _numel_from_meta(n.args[0]),\n",
    "\n",
    "    torch.bitwise_left_shift: lambda n: _numel_from_meta(n),\n",
    "    torch.ops.aten.bitwise_left_shift: lambda n: _numel_from_meta(n),\n",
    "}\n",
    "\n",
    "# ─── nn.Module mapping → flop functions ────────────────────────────────────\n",
    "MODULE_FLOP_MAP: Dict[type, Callable] = {\n",
    "    nn.Conv1d:    lambda m, o: 2 * o[0] * o[1] * o[2] * m.in_channels * m.kernel_size[0],\n",
    "    nn.Conv2d:    lambda m, o: 2 * o[0] * o[1] * o[2] * o[3]\n",
    "                                     * m.in_channels\n",
    "                                     * m.kernel_size[0]\n",
    "                                     * m.kernel_size[1],\n",
    "    nn.Conv3d:    lambda m, o: 2 * o[0] * o[1] * o[2] * o[3] * o[4]\n",
    "                                     * m.in_channels\n",
    "                                     * m.kernel_size[0]\n",
    "                                     * m.kernel_size[1]\n",
    "                                     * m.kernel_size[2],\n",
    "    nn.Linear:    lambda m, o: 2 * o[0] * m.in_features * m.out_features,\n",
    "    nn.BatchNorm1d: lambda m, o: 2 * _numel_from_meta_simple(o),\n",
    "    nn.BatchNorm2d: lambda m, o: 2 * _numel_from_meta_simple(o),\n",
    "    nn.BatchNorm3d: lambda m, o: 2 * _numel_from_meta_simple(o),\n",
    "}\n",
    "\n",
    "# ─── The FX‑based FLOP counter ─────────────────────────────────────────────\n",
    "def fx_count_flops(\n",
    "    model: torch.nn.Module,\n",
    "    inputs: tuple,\n",
    "    custom_aten: Dict[Callable, Callable] = None,\n",
    "    custom_modules: Dict[type, Callable] = None,\n",
    ") -> int:\n",
    "    # 1) Trace & shape‑propagate\n",
    "    if isinstance(model, GraphModule):\n",
    "        gm = model\n",
    "    else:\n",
    "        gm = symbolic_trace(model)\n",
    "    ShapeProp(gm).propagate(*inputs)\n",
    "\n",
    "    aten_map = {**ATEN_FLOP_MAP,    **(custom_aten or {})}\n",
    "    mod_map  = {**MODULE_FLOP_MAP, **(custom_modules or {})}\n",
    "\n",
    "    total_flops = 0\n",
    "    for node in gm.graph.nodes:\n",
    "        # 0) skip zero‑cost\n",
    "        if node.op == \"call_function\" and node.target in _ZERO_COST:\n",
    "            continue\n",
    "\n",
    "        # 1) fused‑function fast path\n",
    "        if node.op == \"call_function\" and getattr(node.target, \"is_fused_function\", False):\n",
    "            ne = _numel_from_meta(node)\n",
    "            total_flops += node.target.flops_per_element * ne\n",
    "            continue\n",
    "\n",
    "        # 2) nn.Modules\n",
    "        if node.op == \"call_module\":\n",
    "            sub = gm.get_submodule(node.target)\n",
    "            for T, fn in mod_map.items():\n",
    "                if isinstance(sub, T):\n",
    "                    total_flops += int(fn(sub, node.meta[\"tensor_meta\"].shape))\n",
    "                    break\n",
    "            continue\n",
    "\n",
    "        # 3) explicit ATen (elementwise, matmul, constant factories, aten.sum/prod)\n",
    "        if node.op == \"call_function\" and node.target in aten_map:\n",
    "            total_flops += int(aten_map[node.target](node))\n",
    "            continue\n",
    "\n",
    "        # 4) reductions via Python‐level sum/prod\n",
    "        if node.op == \"call_function\" and node.target in _SUM_FNS:\n",
    "            # cost = #elements of the input\n",
    "            total_flops += int(np.prod(node.args[0].meta[\"tensor_meta\"].shape))\n",
    "            continue\n",
    "        if node.op == \"call_function\" and node.target in _PROD_FNS:\n",
    "            total_flops += int(np.prod(node.args[0].meta[\"tensor_meta\"].shape))\n",
    "            continue\n",
    "\n",
    "        # 5/6) generic fallback: any other call_*(function|method) that has tensor_meta\n",
    "        if (node.op in (\"call_function\",\"call_method\") and\n",
    "            \"tensor_meta\" in node.meta):\n",
    "            total_flops += _numel_from_meta(node)\n",
    "            continue\n",
    "\n",
    "    return total_flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38faae62-203a-4927-8368-3726b5433508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile_model(\n",
    "    model: nn.Module, \n",
    "    dummy_input: torch.Tensor,  # Take pre-generated input tensor\n",
    "    num_runs: int = 8,\n",
    "    warmup: int = 3\n",
    ") -> Dict[str, float]:\n",
    "    device = dummy_input.device\n",
    "    model = model.to(device).eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(dummy_input)\n",
    "        torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "\n",
    "    # Latency measurement\n",
    "    times = []\n",
    "    for _ in range(num_runs + 3):\n",
    "        if device.type == \"cuda\":\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            torch.cuda.synchronize()\n",
    "            start.record()\n",
    "            _ = model(dummy_input)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(start.elapsed_time(end))\n",
    "        else:\n",
    "            t0 = time.time()\n",
    "            _ = model(dummy_input)\n",
    "            times.append((time.time() - t0) * 1000)\n",
    "    \n",
    "    # Process times\n",
    "    times = sorted(times)\n",
    "    cut = int(len(times) * 0.1)\n",
    "    avg_time = np.mean(times[cut:-cut])\n",
    "\n",
    "    # Memory measurement\n",
    "    peak_mem = 0.0\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "            torch.cuda.synchronize()\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "\n",
    "    # FLOP counting\n",
    "    total_flops = 0\n",
    "    try:\n",
    "        gm = symbolic_trace(model)\n",
    "        fx.passes.shape_prop.ShapeProp(gm).propagate(dummy_input)\n",
    "        total_flops = fx_count_flops(gm, (dummy_input,)) / 1e9\n",
    "    except Exception as e:\n",
    "        print(f\"FLOP counting failed: {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        'avg_time_ms': avg_time,\n",
    "        'peak_mem_mb': peak_mem,\n",
    "        'total_flops_g': total_flops\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d61b782-1f29-4926-b4a6-81dd03419b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.fx as fx\n",
    "\n",
    "# ─── models ──────────────────────────────────────────────────────────────────\n",
    "class DeepElementwiseModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        for _ in range(40):\n",
    "            x = torch.sigmoid(torch.relu(x + 1))\n",
    "        return x.mean()\n",
    "        \n",
    "class EnhancedResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(weights=None)\n",
    "        self.features = nn.Sequential(\n",
    "            base.conv1, base.bn1, base.relu, base.maxpool,\n",
    "            base.layer1, base.layer2\n",
    "        )\n",
    "        self.pattern_conv = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        for _ in range(30):\n",
    "            a = x\n",
    "            b = self.pattern_conv(x)\n",
    "            c = self.pattern_conv(x)\n",
    "            x = a*c + a*b\n",
    "            x = torch.sigmoid(torch.relu(x + 1))\n",
    "        return self.fc(x.mean(dim=[2, 3]))\n",
    "\n",
    "class EnhancedVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.vgg16(weights=None)\n",
    "        self.features = base.features\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # a short elementwise chain\n",
    "        x = torch.tanh(torch.exp(x + 2))\n",
    "        x = torch.sigmoid(x)\n",
    "        x = torch.sqrt(x + 1)\n",
    "        return self.fc(x.mean(dim=[2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8d466da-1447-4f1e-86e2-59c4fc2b668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitShiftTestModel(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        shifted = torch.bitwise_left_shift(x, 1)\n",
    "        return torch.sum(shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa003bc-5957-4f66-9ea1-3d89a892eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewriteTriggerResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet18(weights=None)\n",
    "        self.features = nn.Sequential(*list(backbone.children())[:-1])  # Remove FC\n",
    "        self.head = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, image, B, C):\n",
    "        # === Real path (ResNet on image input) ===\n",
    "        x = self.features(image)  \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # === Rewrite patterns on vector-shaped inputs ===\n",
    "        BIG_B = B.repeat(1, 64) \n",
    "        BIG_C = C.repeat(1, 64)\n",
    "        \n",
    "        for _ in range(100):  # Repeat 100x to amplify FLOP impact\n",
    "\n",
    "            # 1. prod(exp(x)) → exp(sum(x))\n",
    "            _ = torch.prod(torch.exp(BIG_B), dim=1)\n",
    "    \n",
    "            # 2. (B * sqrt(C)) * (sqrt(C) * B)\n",
    "            s = torch.sqrt(BIG_C)\n",
    "            _ = (BIG_B * s) * (s * BIG_B)\n",
    "    \n",
    "            # 3. sum(bitshift(x)) → bitshift(sum(x))\n",
    "            shifted = torch.bitwise_left_shift(BIG_B.to(torch.int32), 2)\n",
    "            summed_int = torch.sum(shifted, dim=1, keepdim=True)\n",
    "            _ = summed_int.float()\n",
    "    \n",
    "            # 4. A * B + A * C → A * (B + C)\n",
    "            _ = BIG_B * BIG_C + BIG_B * BIG_B\n",
    "    \n",
    "            # 5. A + A * B → A * (B + 1)\n",
    "            _ = BIG_B + BIG_B * BIG_C\n",
    "    \n",
    "            # 6. (1/B) * (1/(B*C)) → (1/B)^2 * (1/C)\n",
    "            _ = torch.reciprocal(BIG_B) * torch.reciprocal(BIG_B * BIG_C)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "class RewriteTriggerResNetWrapped(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = x[:, :, :, :224]  # [32,3,224,224]\n",
    "        B = x[:, 0, 0, 224:224+8192]    # [32,8192]\n",
    "        C = x[:, 0, 0, 224+8192:224+16384]  # [32,8192]\n",
    "        return self.base_model(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67ebfce-9cbe-4380-8b8d-08f64bd4a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveRewriteModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A single‑module harness containing all of the algebraic rewrites:\n",
    "      1) Bitshift→Sum\n",
    "      2) Prod(Exp)→Exp(Sum)\n",
    "      3) Recip‑Associative\n",
    "      4) Sqrt‑Associative\n",
    "      5) Distributive A*B + A*C → A*(B+C)\n",
    "      6) Distributive2 A + A*B → A*(1+B)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # Build two helper tensors B and C from x\n",
    "        B = x + 1.0\n",
    "        C = x + 2.0\n",
    "\n",
    "        # 1) Bitshift → Sum\n",
    "        x_int   = x.to(torch.int32)\n",
    "        t1      = torch.bitwise_left_shift(x_int, 1)\n",
    "        sum_int = torch.sum(t1, dim=1, keepdim=True)\n",
    "        p1      = sum_int.float()\n",
    "\n",
    "        # 2) Prod(Exp) → Exp(Sum)\n",
    "        t2 = torch.exp(B)\n",
    "        p2 = torch.prod(t2, dim=1, keepdim=True)\n",
    "\n",
    "        # 3) Recip‑Associative: Recip(A) * Recip(A * B) → (Recip(A))² * Recip(B)\n",
    "        rA  = torch.reciprocal(B)\n",
    "        rAB = torch.reciprocal(B * C)\n",
    "        p3  = rA * rAB\n",
    "\n",
    "        # 4) Sqrt‑Associative: (A*√B)*(√B*C) → A*B*C\n",
    "        sB = torch.sqrt(B)\n",
    "        m1 = x * sB\n",
    "        m2 = sB * C\n",
    "        p4 = m1 * m2\n",
    "\n",
    "        # 5) Distributive: A*B + A*C → A*(B+C)\n",
    "        d5 = x * B + x * C\n",
    "\n",
    "        # 6) Distributive2: A + A*B → A*(1 + B)\n",
    "        d6 = x + x * C\n",
    "\n",
    "        # Combine everything and collapse to a scalar\n",
    "        out = p1 + p2 + p3 + p4 + d5 + d6\n",
    "        return out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf83167d-78c8-4a0b-ae58-c2bed0c2521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.fx import symbolic_trace\n",
    "\n",
    "def evaluate_all_models_with_breakdown():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    summary = []\n",
    "\n",
    "    model_configs = [\n",
    "        {\n",
    "            \"name\": \"RewriteResNet\",\n",
    "            \"model\": RewriteTriggerResNetWrapped(RewriteTriggerResNet()).to(device),\n",
    "            \"make_input\": lambda: torch.cat([\n",
    "                torch.randn(32, 3, 224, 224, device=device),\n",
    "                torch.randint(0, 16, (32, 1, 1, 16384), device=device).float()\n",
    "                    .expand(-1, 3, 224, -1)\n",
    "            ], dim=3),\n",
    "            \"passes\": [\n",
    "                (\"Commutative\",        CommutativePass),\n",
    "                (\"SqrtAssociative\",    SqrtAssociativePass),\n",
    "                (\"BitShift→Sum\",       swap_bitshift_reducesum),\n",
    "                (\"Distributive\",       DistributiveRulePass),\n",
    "                (\"Distributive2\",      DistributiveRule2Pass),\n",
    "                (\"Recip‑Assoc\",        swap_recip_associative),\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ComprehensiveRewrite\",\n",
    "            \"model\": ComprehensiveRewriteModel().to(device),\n",
    "            \"make_input\": lambda: torch.randn(32, 3, 224, 224, device=device),\n",
    "            \"passes\": [\n",
    "                (\"Commutative\",        CommutativePass),\n",
    "                (\"SqrtAssociative\",    SqrtAssociativePass),\n",
    "                (\"BitShift→Sum\",       swap_bitshift_reducesum),\n",
    "                (\"Distributive\",       DistributiveRulePass),\n",
    "                (\"Distributive2\",      DistributiveRule2Pass),\n",
    "                (\"Recip‑Assoc\",        swap_recip_associative),\n",
    "            ]\n",
    "        },\n",
    "        # summary‑only models\n",
    "        {\n",
    "            \"name\": \"EnhancedResNet\",\n",
    "            \"model\": EnhancedResNet().to(device),\n",
    "            \"make_input\": lambda: torch.randn(32, 3, 224, 224, device=device),\n",
    "            \"passes\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EnhancedVGG\",\n",
    "            \"model\": EnhancedVGG().to(device),\n",
    "            \"make_input\": lambda: torch.randn(32, 3, 224, 224, device=device),\n",
    "            \"passes\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DeepElementwise\",\n",
    "            \"model\": DeepElementwiseModel().to(device),\n",
    "            \"make_input\": lambda: torch.randn(32, 3, 224, 224, device=device),\n",
    "            \"passes\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"BitShiftTest\",\n",
    "            \"model\": BitShiftTestModel().to(device),\n",
    "            \"make_input\": lambda: torch.randint(0,16,(32,3,224,224),\n",
    "                                                device=device, dtype=torch.int32),\n",
    "            \"passes\": None\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for cfg in model_configs:\n",
    "        name    = cfg[\"name\"]\n",
    "        model   = cfg[\"model\"].eval()\n",
    "        dummy   = cfg[\"make_input\"]()\n",
    "\n",
    "        # 1) baseline\n",
    "        gm0       = symbolic_trace(model)\n",
    "        base_stat = profile_model(gm0, dummy)\n",
    "\n",
    "        # 2) global optimize_graph\n",
    "        gm1       = optimize_graph(symbolic_trace(model))\n",
    "        opt_stat  = profile_model(gm1, dummy)\n",
    "\n",
    "        if cfg[\"passes\"] is None:\n",
    "            # just gather for final summary\n",
    "            Δf = 100 * (base_stat[\"total_flops_g\"] - opt_stat[\"total_flops_g\"]) / base_stat[\"total_flops_g\"]\n",
    "            Δt = 100 * (base_stat[\"avg_time_ms\"]   - opt_stat[\"avg_time_ms\"])   / base_stat[\"avg_time_ms\"]\n",
    "            Δm = 100 * (base_stat[\"peak_mem_mb\"]   - opt_stat[\"peak_mem_mb\"])   / base_stat[\"peak_mem_mb\"]\n",
    "            summary.append({\n",
    "                \"Model\":        name,\n",
    "                \"Baseline FLOPs\": base_stat[\"total_flops_g\"],\n",
    "                \"Opt FLOPs\":      opt_stat[\"total_flops_g\"],\n",
    "                \"ΔFLOPs %\":       f\"{Δf:+.1f}%\",\n",
    "                \"Baseline ms\":    base_stat[\"avg_time_ms\"],\n",
    "                \"Opt ms\":         opt_stat[\"avg_time_ms\"],\n",
    "                \"ΔLat %\":         f\"{Δt:+.1f}%\",\n",
    "                \"Baseline MB\":    base_stat[\"peak_mem_mb\"],\n",
    "                \"Opt MB\":         opt_stat[\"peak_mem_mb\"],\n",
    "                \"ΔMem %\":         f\"{Δm:+.1f}%\"\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            # detailed per‑pass breakdown\n",
    "            print(f\"\\n--- {name} ---\")\n",
    "            print(f\"Baseline  →  FLOPs {base_stat['total_flops_g']:.2f}G   \"\n",
    "                  f\"Latency {base_stat['avg_time_ms']:.2f}ms   \"\n",
    "                  f\"Mem {base_stat['peak_mem_mb']:.1f}MB\\n\")\n",
    "\n",
    "            for pass_name, Pass in cfg[\"passes\"]:\n",
    "                gm_p   = symbolic_trace(model)\n",
    "                gm_opt = Pass()(gm_p) if isinstance(Pass, type) else Pass(gm_p)\n",
    "                pstat  = profile_model(gm_opt, dummy)\n",
    "\n",
    "                Δf_p = 100 * (base_stat[\"total_flops_g\"] - pstat[\"total_flops_g\"]) / base_stat[\"total_flops_g\"]\n",
    "                Δt_p = 100 * (base_stat[\"avg_time_ms\"]   - pstat[\"avg_time_ms\"])   / base_stat[\"avg_time_ms\"]\n",
    "                Δm_p = 100 * (base_stat[\"peak_mem_mb\"]   - pstat[\"peak_mem_mb\"])   / base_stat[\"peak_mem_mb\"]\n",
    "\n",
    "                print(f\"{pass_name:<18}\"\n",
    "                      f\" FLOPs {pstat['total_flops_g']:.2f}G ({Δf_p:+.1f}%)  \"\n",
    "                      f\"Time {pstat['avg_time_ms']:.2f}ms ({Δt_p:+.1f}%)  \"\n",
    "                      f\"Mem {pstat['peak_mem_mb']:.1f}MB ({Δm_p:+.1f}%)\")\n",
    "\n",
    "    # 3) finally print the summary for the rest\n",
    "    if summary:\n",
    "        df = pd.DataFrame(summary).set_index(\"Model\")\n",
    "        print(\"\\n=== Summary for the other models ===\")\n",
    "        display(df.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b0357c-26be-47d3-b077-aa1cda05d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RewriteResNet ---\n",
      "Baseline  →  FLOPs 146.49G   Latency 195.10ms   Mem 2453.1MB\n",
      "\n",
      "Commutative        FLOPs 144.81G (+1.1%)  Time 187.85ms (+3.7%)  Mem 2453.1MB (+0.0%)\n",
      "SqrtAssociative    FLOPs 143.13G (+2.3%)  Time 175.59ms (+10.0%)  Mem 2453.1MB (+0.0%)\n",
      "BitShift→Sum       FLOPs 144.81G (+1.1%)  Time 188.02ms (+3.6%)  Mem 2453.1MB (+0.0%)\n",
      "Distributive       FLOPs 144.81G (+1.1%)  Time 187.91ms (+3.7%)  Mem 2453.1MB (+0.0%)\n",
      "Distributive2      FLOPs 146.49G (+0.0%)  Time 199.65ms (-2.3%)  Mem 2453.1MB (+0.0%)\n",
      "Recip‑Assoc        FLOPs 146.49G (+0.0%)  Time 191.95ms (+1.6%)  Mem 2453.1MB (+0.0%)\n",
      "\n",
      "--- ComprehensiveRewrite ---\n",
      "Baseline  →  FLOPs 0.12G   Latency 0.88ms   Mem 252.3MB\n",
      "\n",
      "Commutative        FLOPs 0.12G (+2.7%)  Time 0.89ms (-1.0%)  Mem 252.3MB (+0.0%)\n",
      "SqrtAssociative    FLOPs 0.11G (+8.1%)  Time 0.83ms (+5.4%)  Mem 252.3MB (+0.0%)\n",
      "BitShift→Sum       FLOPs 0.12G (+2.7%)  Time 0.89ms (-1.4%)  Mem 252.3MB (+0.0%)\n",
      "Distributive       FLOPs 0.11G (+4.1%)  Time 0.86ms (+2.4%)  Mem 252.3MB (+0.0%)\n",
      "Distributive2      FLOPs 0.12G (+0.0%)  Time 0.91ms (-2.8%)  Mem 252.3MB (+0.0%)\n",
      "Recip‑Assoc        FLOPs 0.12G (+0.0%)  Time 0.88ms (+0.2%)  Mem 252.3MB (+0.0%)\n",
      "\n",
      "=== Summary for the other models ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline FLOPs</th>\n",
       "      <th>Opt FLOPs</th>\n",
       "      <th>ΔFLOPs %</th>\n",
       "      <th>Baseline ms</th>\n",
       "      <th>Opt ms</th>\n",
       "      <th>ΔLat %</th>\n",
       "      <th>Baseline MB</th>\n",
       "      <th>Opt MB</th>\n",
       "      <th>ΔMem %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EnhancedResNet</th>\n",
       "      <td>508.11</td>\n",
       "      <td>508.02</td>\n",
       "      <td>+0.0%</td>\n",
       "      <td>11.57</td>\n",
       "      <td>10.76</td>\n",
       "      <td>+7.0%</td>\n",
       "      <td>2390.60</td>\n",
       "      <td>2023.10</td>\n",
       "      <td>+15.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnhancedVGG</th>\n",
       "      <td>982.19</td>\n",
       "      <td>982.19</td>\n",
       "      <td>+0.0%</td>\n",
       "      <td>15.27</td>\n",
       "      <td>15.29</td>\n",
       "      <td>-0.1%</td>\n",
       "      <td>3531.38</td>\n",
       "      <td>3531.38</td>\n",
       "      <td>+0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DeepElementwise</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>+0.0%</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.08</td>\n",
       "      <td>+2.3%</td>\n",
       "      <td>166.18</td>\n",
       "      <td>166.18</td>\n",
       "      <td>+0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BitShiftTest</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>+50.0%</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>+16.7%</td>\n",
       "      <td>184.56</td>\n",
       "      <td>166.18</td>\n",
       "      <td>+10.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Baseline FLOPs  Opt FLOPs ΔFLOPs %  Baseline ms  Opt ms  \\\n",
       "Model                                                                      \n",
       "EnhancedResNet           508.11     508.02    +0.0%        11.57   10.76   \n",
       "EnhancedVGG              982.19     982.19    +0.0%        15.27   15.29   \n",
       "DeepElementwise            0.58       0.58    +0.0%         2.13    2.08   \n",
       "BitShiftTest               0.01       0.00   +50.0%         0.12    0.10   \n",
       "\n",
       "                 ΔLat %  Baseline MB   Opt MB  ΔMem %  \n",
       "Model                                                  \n",
       "EnhancedResNet    +7.0%      2390.60  2023.10  +15.4%  \n",
       "EnhancedVGG       -0.1%      3531.38  3531.38   +0.0%  \n",
       "DeepElementwise   +2.3%       166.18   166.18   +0.0%  \n",
       "BitShiftTest     +16.7%       184.56   166.18  +10.0%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_all_models_with_breakdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16f6ab-f871-4c83-9b18-642147b1173c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
